<!DOCTYPE html><html lang="zh-Hans" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Spark环境搭建（一）Local模式 | 学习大数据</title><meta name="keywords" content="Spark环境搭建"><meta name="author" content="XiaoQu"><meta name="copyright" content="XiaoQu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Spark环境搭建（一）Local模式        Term Meaning    Application User program built on Spark. Consists of a driver program and executors on the cluster.   Application jar A jar containing the user’s Spark appli">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark环境搭建（一）Local模式">
<meta property="og:url" content="http://www.studybigdata.cn/Spark/Spark_Env_Local/index.html">
<meta property="og:site_name" content="学习大数据">
<meta property="og:description" content="Spark环境搭建（一）Local模式        Term Meaning    Application User program built on Spark. Consists of a driver program and executors on the cluster.   Application jar A jar containing the user’s Spark appli">
<meta property="og:locale">
<meta property="og:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7">
<meta property="article:published_time" content="2022-03-23T07:55:26.000Z">
<meta property="article:modified_time" content="2022-11-18T08:25:12.423Z">
<meta property="article:author" content="XiaoQu">
<meta property="article:tag" content="Spark环境搭建">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://www.studybigdata.cn/Spark/Spark_Env_Local/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="baidu-site-verification" content="code-qvYtDCUlcS"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?057fccfc2b1bef4c0d79224d80660be0";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark环境搭建（一）Local模式',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-11-18 16:25:12'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.1.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/dog.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">54</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">29</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">16</div></a></div></div><hr/></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">学习大数据</a></span><div id="menus"><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Spark环境搭建（一）Local模式</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-03-23T07:55:26.000Z" title="Created 2022-03-23 15:55:26">2022-03-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-11-18T08:25:12.423Z" title="Updated 2022-11-18 16:25:12">2022-11-18</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark环境搭建（一）Local模式"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div><article class="post-content" id="article-container"><h1 style="color:red; text-align:center;color:red;">Spark环境搭建（一）Local模式</h1>



<p><img src="/img/Spark/cluster-overview.png" alt="Spark cluster components"></p>
<table>
<thead>
<tr>
<th align="left">Term</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>Application</code></td>
<td align="left">User program built on Spark. Consists of a <em>driver program</em> and <em>executors</em> on the cluster.</td>
</tr>
<tr>
<td align="left">Application jar</td>
<td align="left">A jar containing the user’s Spark application. In some cases users will want to create an “uber jar” containing their application along with its dependencies. The user’s jar should never include Hadoop or Spark libraries, however, these will be added at runtime.</td>
</tr>
<tr>
<td align="left"><code>Driver</code></td>
<td align="left">The process running the <code>main() function</code> of the application and <code>creating the SparkContext</code></td>
</tr>
<tr>
<td align="left">Cluster manager</td>
<td align="left">An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN)</td>
</tr>
<tr>
<td align="left">Deploy mode</td>
<td align="left">Distinguishes where the driver process runs. In “cluster” mode, the framework launches the driver inside of the cluster. In “client” mode, the submitter launches the driver outside of the cluster.</td>
</tr>
<tr>
<td align="left">Worker node</td>
<td align="left">Any node that can run application code in the cluster</td>
</tr>
<tr>
<td align="left"><code>Executor</code></td>
<td align="left">A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors.</td>
</tr>
<tr>
<td align="left">Task</td>
<td align="left">A unit of work that will be sent to one executor</td>
</tr>
<tr>
<td align="left">Job</td>
<td align="left">A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. <code>save</code>, <code>collect</code>); you’ll see this term used in the driver’s logs.</td>
</tr>
<tr>
<td align="left">Stage</td>
<td align="left">Each job gets divided into smaller sets of tasks called <em>stages</em> that depend on each other (similar to the map and reduce stages in MapReduce); you’ll see this term used in the driver’s logs.</td>
</tr>
</tbody></table>
<h1 id="环境部署"><a href="#环境部署" class="headerlink" title="环境部署"></a>环境部署</h1><blockquote>
<p><strong>官方文档</strong></p>
<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/2.4.8/">https://spark.apache.org/docs/2.4.8/</a></p>
<p><strong>下载地址</strong></p>
<p><a target="_blank" rel="noopener" href="https://archive.apache.org/dist/spark/spark-2.4.8/">https://archive.apache.org/dist/spark/spark-2.4.8/</a></p>
<p><strong>安装目录</strong></p>
<p><code>/opt/bigdata/spark</code></p>
<p><strong>部署规划</strong></p>
<p><code>node0</code>：Local (Linux File System &amp; HDFS)</p>
<p><code>node1</code> <code>node2</code> <code>node3</code>  配置为<code>spark</code>集群。</p>
</blockquote>
<iframe id="embed_dom" name="embed_dom" frameborder="0" style="display:block;width:525px; height:400px;" src="https://www.processon.com/embed/62083c165653bb06de331e3c"></iframe>

<p><img src="/img/Spark/image-20220908143725377.png" alt="image-20220908143725377"></p>
<p>在<code>/opt/</code>目录下创建<code>bigdata</code>目录，并修改目录的所有者和所属组为<code>zhangsan</code>，我们将<code>spark</code>安装到该目录中。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建/opt/bigdata/spark文件夹</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">!!! 注意： 因为普通用户没有权限在/opt/目录写数据，所有本次操作使用的是root账户。</span></span><br><span class="line">[root@node0 ~]# mkdir /opt/bigdata/spark</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改/opt/bigdata/spark文件夹及子文件夹的所有者所属组为zhangsan</span></span><br><span class="line">[root@node0 ~]# chown -R zhangsan:zhangsan /opt/bigdata/spark</span><br><span class="line">[root@node0 ~]# ls -al /opt/bigdata/</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x. 3 zhangsan zhangsan 19 Feb 15 08:38 .</span><br><span class="line">drwxr-xr-x. 3 root   root   21 Feb 15 08:38 ..</span><br><span class="line">drwxr-xr-x. 2 zhangsan zhangsan  6 Feb 15 08:38 spark</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">退出root账号</span></span><br><span class="line">[root@node0 ~]# exit</span><br><span class="line">exit</span><br></pre></td></tr></table></figure>

<p>使用<code>xftp</code>等文件传输工具，将<code>spark</code>安装包上传到<code>CentOS 7</code>系统的<code>/opt/bigdata/spark</code>目录中。</p>
<hr>
<h2 id="本地运行"><a href="#本地运行" class="headerlink" title="本地运行"></a>本地运行</h2><h3 id="读写Linux"><a href="#读写Linux" class="headerlink" title="读写Linux"></a>读写Linux</h3><p>本地运行模式开箱即用，只需要<a href=""><code>Java</code>环境</a>，无需其他任何配置。此时，只能读写本机<code>Linux文件系统</code>中的文件，无法读写<code>HDFS</code>。</p>
<h4 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[zhangsan@node0 ~]$ cd /opt/bigdata/spark/</span><br><span class="line">[zhangsan@node0 spark]$ tar -zxf spark-2.4.8-bin-hadoop2.7.tgz</span><br><span class="line">[zhangsan@node0 spark]$ ll</span><br><span class="line">total 230372</span><br><span class="line">drwxr-xr-x. 13 zhangsan zhangsan       211 May  8  2021 spark-2.4.8-bin-hadoop2.7</span><br><span class="line">-rw-rw-r--.  1 zhangsan zhangsan 235899716 Feb 15 08:42 spark-2.4.8-bin-hadoop2.7.tgz</span><br><span class="line">[zhangsan@node0 spark]$ ln -s spark-2.4.8-bin-hadoop2.7 default</span><br><span class="line">[zhangsan@node0 spark]$ ll</span><br><span class="line">total 230372</span><br><span class="line">lrwxrwxrwx.  1 zhangsan zhangsan        25 Feb 15 08:42 default -&gt; spark-2.4.8-bin-hadoop2.7</span><br><span class="line">drwxr-xr-x. 13 zhangsan zhangsan       211 May  8  2021 spark-2.4.8-bin-hadoop2.7</span><br><span class="line">-rw-rw-r--.  1 zhangsan zhangsan 235899716 Feb 15 08:42 spark-2.4.8-bin-hadoop2.7.tgz</span><br></pre></td></tr></table></figure>

<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><p>无需配置。</p>
<h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[zhangsan@node0 spark]$ cd default/bin/</span><br><span class="line">[zhangsan@node0 bin]$ rm -rf *.cmd # 删除Windows平台的脚本（可选）</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动 spark-shell 交互式环境（scala语言）</span></span><br><span class="line">[zhangsan@node0 bin]$ ./spark-shell --master local[*]</span><br><span class="line">22/02/13 12:50:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">可通过此 web 界面查看 spark job 的运行情况</span></span><br><span class="line">Spark context Web UI available at http://node0:4040</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">local</span>[*] 表示使用所有计算资源</span></span><br><span class="line">Spark context available as &#x27;sc&#x27; (master = local[*], app id = local-1644727904419).</span><br><span class="line">Spark session available as &#x27;spark&#x27;.</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &#x27;_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   version 2.4.8</span><br><span class="line">      /_/</span><br><span class="line">         </span><br><span class="line">Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_202)</span><br></pre></td></tr></table></figure>

<p>可通过spark web界面查job的启动情况，网页中看不到job的运行情况，因为我们还没有执行spark job。</p>
<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://node0:4040</span><br></pre></td></tr></table></figure>

<p>本地模式只有一个进程启动。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[zhangsan@node0 ~]$ jps</span><br><span class="line">18371 SparkSubmit</span><br><span class="line">20020 Jps</span><br><span class="line">[zhangsan@node0 ~]$ </span><br></pre></td></tr></table></figure>

<h4 id="Master参数"><a href="#Master参数" class="headerlink" title="Master参数"></a>Master参数</h4><table>
<thead>
<tr>
<th>–master参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><code>local</code></td>
<td>使用<code>一个Worker线程</code>本地化运行Spark</td>
</tr>
<tr>
<td><code>local[k]</code></td>
<td>使用<code>K个Worker线程</code>本地化运行Spark</td>
</tr>
<tr>
<td><code>local[*]</code></td>
<td>使用<code>* 个Worker线程</code>本地化运行Spark(<code>* =机器的CPU核数</code>)（默认）</td>
</tr>
<tr>
<td><code>spark://HOST:PORT</code></td>
<td>连接到指定的<code>Standalone集群</code>。<code>HOST</code>参数是<code>Spark Master</code>的<code>hostname</code>或<code>IP</code>，默认端口是<code>7077。</code></td>
</tr>
<tr>
<td><code>mesos://HOST:PORT</code></td>
<td>连接到指定的<code>Mesos集群</code>。<code>HOST</code>参数是<code>Moses Master</code>的<code>hostname</code>或<code>IP</code>，默认端口是<code>5050</code>  。</td>
</tr>
<tr>
<td><code>yarn</code></td>
<td>默认以<code>客户端模式</code>连接到<code>YARN集群</code>，集群位置由环境变量<code>YARN_CONF_DIR</code>决定 。</td>
</tr>
</tbody></table>
<p><code>Spark2.0</code>以前，<code>yarn</code>分为<code>yarn-client</code>与<code>yarn-cluster</code></p>
<p><code>Spark2.0</code>以后，设置<code>--deploy-mode=[client/cluster]</code>以不同模式连接到<code>YARN集群</code></p>
<h4 id="案例-wordcount"><a href="#案例-wordcount" class="headerlink" title="案例 - wordcount"></a>案例 - wordcount</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hello</span><br><span class="line">study bigdata</span><br><span class="line">hello study bigdata</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> wordcount = sc.textFile(<span class="string">&quot;/home/zhangsan/bigdata.txt&quot;</span>).flatMap(line =&gt; line.split(<span class="string">&quot; &quot;</span>)).map(word =&gt; (word,<span class="number">1</span>)).reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">scala&gt; wordcount.collect()</span><br><span class="line">res0: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((hello,<span class="number">2</span>), (bigdata,<span class="number">2</span>), (study,<span class="number">2</span>))         </span><br></pre></td></tr></table></figure>

<p>打开web <a target="_blank" rel="noopener" href="http://node0:4040/">http://node0:4040</a> 界面，可以查看<code>Job</code>的运行情况，此地址为<code>driver</code>执行任务的查看地址。</p>
<p><img src="/img/Spark/image-20220213132758743.png" alt="image-20220213132758743"></p>
<p>可以看到该<code>Job</code>包括两个<code>stage</code></p>
<p><img src="/img/Spark/image-20220213133658467.png" alt="image-20220213133658467"></p>
<p>直接解压运行，<code>spark</code>只能读写本地<code>Linux</code>文件系统；接下来我们配置<code>spark</code>，让其能够读写<code>HDFS</code>。</p>
<h3 id="读写HDFS"><a href="#读写HDFS" class="headerlink" title="读写HDFS"></a>读写HDFS</h3><p>接下来的实验中，我们需要使用<code>Spark</code>读写<code>HDFS</code>，因此，我们需要完成<a href=""><code>Hadoop</code>伪分布式环境的搭建</a>。</p>
<h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><p>Spark配置文件<code>spark-env.sh</code>中添加如下行即可。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进入spark配置文件存放目录</span></span><br><span class="line">[zhangsan@node0 ~]$ cd /opt/bigdata/spark/default/conf/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">重命名配置文件模板</span></span><br><span class="line">[zhangsan@node0 conf]$ mv spark-env.sh.template spark-env.sh</span><br><span class="line"></span><br><span class="line">[zhangsan@node0 conf]$ vim spark-env.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改 HADOOP_CONF_DIR配置项</span></span><br><span class="line">HADOOP_CONF_DIR=/opt/bigdata/hadoop/default/etc/hadoop</span><br></pre></td></tr></table></figure>

<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><h5 id="格式化名称节点"><a href="#格式化名称节点" class="headerlink" title="格式化名称节点"></a>格式化名称节点</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zhangsan@node0 ~]$ hadoop namenode -format</span><br></pre></td></tr></table></figure>

<h5 id="启动Hadoop"><a href="#启动Hadoop" class="headerlink" title="启动Hadoop"></a>启动Hadoop</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zhangsan@node0 ~]$ start-all.sh </span><br></pre></td></tr></table></figure>

<h5 id="创建文件夹"><a href="#创建文件夹" class="headerlink" title="创建文件夹"></a>创建文件夹</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zhangsan@node0 ~]$ hdfs dfs -mkdir /input</span><br></pre></td></tr></table></figure>

<h5 id="上传测试数据"><a href="#上传测试数据" class="headerlink" title="上传测试数据"></a>上传测试数据</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zhangsan@node0 ~]$ hdfs dfs -put bigdata.txt /input</span><br></pre></td></tr></table></figure>

<h5 id="wordcount"><a href="#wordcount" class="headerlink" title="wordcount"></a>wordcount</h5><p><code>$SPARK_HOME/bin</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> wordcount = sc.textFile(<span class="string">&quot;hdfs:///input/bigdata.txt&quot;</span>).flatMap(line =&gt; line.split(<span class="string">&quot; &quot;</span>)).map(word =&gt; (word,<span class="number">1</span>)).reduceByKey(_+_)</span><br><span class="line">wordcount: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">11</span>] at reduceByKey at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; wordcount.collect()</span><br><span class="line">res3: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((hello,<span class="number">2</span>), (bigdata,<span class="number">2</span>), (study,<span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<h5 id="退出"><a href="#退出" class="headerlink" title="退出"></a>退出</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; :quit</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">XiaoQu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://www.studybigdata.cn/Spark/Spark_Env_Local/">http://www.studybigdata.cn/Spark/Spark_Env_Local/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">Spark环境搭建</a></div><div class="post_share"><div class="social-share" data-image="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/HBase/HBase_Query/"><img class="prev-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">HBase 数据查询</div></div></a></div><div class="next-post pull-right"><a href="/Spark/Spark_Env_YARN/"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Spark环境搭建（三）Spark On YARN模式</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/Spark/Spark_Env_YARN/" title="Spark环境搭建（三）Spark On YARN模式"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-23</div><div class="title">Spark环境搭建（三）Spark On YARN模式</div></div></a></div><div><a href="/Spark/Spark_Env_Windows_Dev/" title="Spark环境搭建（四）Spark开发环境搭建"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-23</div><div class="title">Spark环境搭建（四）Spark开发环境搭建</div></div></a></div><div><a href="/Spark/Spark_Env_Standalone/" title="Spark环境搭建（二）Standalone模式"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-23</div><div class="title">Spark环境搭建（二）Standalone模式</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/dog.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">XiaoQu</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">54</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">29</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">16</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/A-stranger"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">Spark环境搭建（一）Local模式</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2"><span class="toc-number">2.</span> <span class="toc-text">环境部署</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C"><span class="toc-number">2.1.</span> <span class="toc-text">本地运行</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%86%99Linux"><span class="toc-number">2.1.1.</span> <span class="toc-text">读写Linux</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E5%8E%8B"><span class="toc-number">2.1.1.1.</span> <span class="toc-text">解压</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE"><span class="toc-number">2.1.1.2.</span> <span class="toc-text">配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8"><span class="toc-number">2.1.1.3.</span> <span class="toc-text">启动</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Master%E5%8F%82%E6%95%B0"><span class="toc-number">2.1.1.4.</span> <span class="toc-text">Master参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B-wordcount"><span class="toc-number">2.1.1.5.</span> <span class="toc-text">案例 - wordcount</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%86%99HDFS"><span class="toc-number">2.1.2.</span> <span class="toc-text">读写HDFS</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">配置文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">测试</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A0%BC%E5%BC%8F%E5%8C%96%E5%90%8D%E7%A7%B0%E8%8A%82%E7%82%B9"><span class="toc-number">2.1.2.2.1.</span> <span class="toc-text">格式化名称节点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8Hadoop"><span class="toc-number">2.1.2.2.2.</span> <span class="toc-text">启动Hadoop</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9"><span class="toc-number">2.1.2.2.3.</span> <span class="toc-text">创建文件夹</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%8A%E4%BC%A0%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE"><span class="toc-number">2.1.2.2.4.</span> <span class="toc-text">上传测试数据</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#wordcount"><span class="toc-number">2.1.2.2.5.</span> <span class="toc-text">wordcount</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%80%80%E5%87%BA"><span class="toc-number">2.1.2.2.6.</span> <span class="toc-text">退出</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/Hadoop-deplpy-3-0/" title="Hadoop_deplpy_3.0">Hadoop_deplpy_3.0</a><time datetime="2022-11-29T08:09:17.000Z" title="Created 2022-11-29 16:09:17">2022-11-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/Spark-RDD/" title="Spark_RDD编程">Spark_RDD编程</a><time datetime="2022-11-19T00:47:03.000Z" title="Created 2022-11-19 08:47:03">2022-11-19</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/Kettle/Kettle_Job/" title="kettle - Kettle作业设计">kettle - Kettle作业设计</a><time datetime="2022-11-17T22:34:26.000Z" title="Created 2022-11-18 06:34:26">2022-11-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/Scala/Scala_Object_Oriented_Programming/" title="Scala面向对象编程">Scala面向对象编程</a><time datetime="2022-11-05T22:31:26.000Z" title="Created 2022-11-06 06:31:26">2022-11-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/Kettle/Kettle_Clean_Code_Formulate/" title="kettle - 数据清理之公式清理">kettle - 数据清理之公式清理</a><time datetime="2022-11-02T22:34:26.000Z" title="Created 2022-11-03 06:34:26">2022-11-03</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By XiaoQu</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>